{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### WEB SCRAPPING \n",
    "Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning. \n",
    "Follow these steps to build a web scraping project from scratch using Python and its ecosystem of libraries:\n",
    "\n",
    "1. Pick a website and describe your objective\n",
    "2. Use the requests library to download web pages\n",
    "3. Use Beautiful Soup to parse and extract information\n",
    "4. Create CSV file(s) with the extracted information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps we'll follow:\n",
    "\n",
    "- We're going to scrape https://flipkart.com/search?q=laptop&\n",
    "- We will define the number of pages needed to be scraped\n",
    "- We'll get a list of laptops. For each laptop, we'll get laptop name, laptop price, laptop rating and laptop page URL\n",
    "- For each topic we'll create a CSV file in the following format:\n",
    "\n",
    "```\n",
    "prod name, prod rate, prod price, prod link\n",
    "HP 14s Core i3 11th Gen - (8 GB/256 GB SSD/Windows 10 Home) 14s- DY2501TU Thin and Light Laptop,\n",
    "4.4,\n",
    "\"â‚¹41,890\",\n",
    "www.flipkart.com/hp-14s-core-i3-11th-gen-8-gb-256-gb-ssd-windows-10-home-14s-dy2501tu-thin-light-laptop/p/itmdce0f13dd3a4e?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_link = \"https://www.flipkart.com/search?q=laptop&page=\"\n",
    "\n",
    "\n",
    "def get_document(page):\n",
    "    # To get the html page \n",
    "    res = requests.get(laptop_link + str(page))\n",
    "    if res.status_code != 200:\n",
    "        raise Exception(\"Failed to load page {}\".format(laptop_link))\n",
    "    # Parsing it with beautiful soup\n",
    "    document = BeautifulSoup(res.content, \"html.parser\")\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of pages needed: \n",
      "4\n"
     ]
    }
   ],
   "source": [
    "prod_name_list = []\n",
    "prod_rate_list = []\n",
    "prod_price_list = []\n",
    "prod_link_list = []\n",
    "\n",
    "print(\"Enter the number of pages needed: \")\n",
    "pages = input()\n",
    "for page in range(1, int(pages) + 1):\n",
    "    document = get_document(page)\n",
    "\n",
    "    # To get the name of product\n",
    "    prod_name = \"_4rR01T\"\n",
    "    product_name_tags = document.find_all(\"div\", {\"class\": prod_name})\n",
    "\n",
    "    # To get the rating of the product\n",
    "    prod_rating_selection_class = \"_3LWZlK\"\n",
    "    prod_rating_tag = document.find_all(\"div\", {\"class\": prod_rating_selection_class})\n",
    "\n",
    "    # To get the price of the product\n",
    "    prod_price_selection_price = \"_30jeq3 _1_WHN1\"  # class id\n",
    "    prod_price_tags = document.find_all(\"div\", {\"class\": prod_price_selection_price})\n",
    "\n",
    "    # To get the link of the product\n",
    "    prod_link_selection_class = \"_1fQZEK\"\n",
    "    prod_link_tags = document.find_all(\"a\", {\"class\": prod_link_selection_class})\n",
    "\n",
    "    for prod_name, prod_rate, prod_price, prod_link in zip(product_name_tags, prod_rating_tag, prod_price_tags, prod_link_tags):\n",
    "        prod_name_list.append(prod_name.text)\n",
    "        prod_rate_list.append(prod_rate.text)\n",
    "        prod_price_list.append(prod_price.text)\n",
    "        prod_link_list.append(\"www.flipkart.com\" + prod_link[\"href\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\"product name\": prod_name_list, \"product rate\": prod_rate_list, \"product price\": prod_price_list, \"product link\": prod_link_list}\n",
    "laptop_df = pd.DataFrame(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_df.to_csv(\"laptop_price.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
